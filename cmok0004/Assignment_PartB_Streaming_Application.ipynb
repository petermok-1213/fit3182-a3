{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:48.484584Z",
     "start_time": "2023-06-05T09:12:48.182103Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from json import loads, JSONDecodeError\n",
    "from datetime import datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\"\"\"\n",
    "    Convert lat-lng from tuple of floating points to tuple of binary string presentations of the values\n",
    "    !!!IMPORTANT!!!\n",
    "        To avoid converting floating points to binary directly, all values are x100 and converted to integer\n",
    "        THE FINAL VALUE IS 100X THE ORIGINAL VALUE (but it doesnt matter for our use case)\n",
    "    !!!END OF IMPORTANT!!!\n",
    "\"\"\"\n",
    "def latlng_to_binstr(lat_lng: Tuple[float, float]) -> Tuple[str, str]:\n",
    "    # float(25.125), float(10.13) -> int(25125), int(10130) -> '0b10101100`', '0b11011010' -> '10101100', '11011010'\n",
    "    return bin(int(lat_lng[0]*100))[2:], bin(int(lat_lng[1]*100))[2:]\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are close together\n",
    "                                                          within 3 precision\n",
    "\"\"\"\n",
    "def are_close(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(3):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are the same\n",
    "                                                          within 5 precision\n",
    "\"\"\"\n",
    "def are_same(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(5):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:49.903579Z",
     "start_time": "2023-06-05T09:12:49.892785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "TOPIC_NAME = \"topic_1\"\n",
    "HOST_IP = \"192.168.20.6\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:50.812405Z",
     "start_time": "2023-06-05T09:12:50.810693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/petermok/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/petermok/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-867621aa-051a-43d2-ae91-5a245bf1ce7d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1446ms :: artifacts dl 135ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-867621aa-051a-43d2-ae91-5a245bf1ce7d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/40ms)\n",
      "23/06/05 19:12:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    EXCEPT errno 61 connection refused:\n",
    "        RESTART ipynb kernel\n",
    "\"\"\"\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:00.368818Z",
     "start_time": "2023-06-05T09:12:51.428138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "p1_topic_name = \"climate_producer\"\n",
    "p1_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p1_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p1_stream_df.printSchema()\n",
    "\n",
    "p1_output_stream_df = (\n",
    "    p1_stream_df\n",
    "    .select(p1_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p1_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T20:44:29.172702Z",
     "start_time": "2023-06-05T20:44:29.030579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2_topic_name = \"aqua_producer\"\n",
    "p2_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p2_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p2_stream_df.printSchema()\n",
    "\n",
    "p2_output_stream_df = (\n",
    "    p2_stream_df\n",
    "    .select(p2_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p2_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:17.103367Z",
     "start_time": "2023-06-05T09:13:16.989060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p3_topic_name = \"terra_producer\"\n",
    "p3_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p3_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p3_stream_df.printSchema()\n",
    "\n",
    "p3_output_stream_df = (\n",
    "    p3_stream_df\n",
    "    .select(p3_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p3_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:18.919084Z",
     "start_time": "2023-06-05T09:13:18.788503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "class ClimateWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.producer = None\n",
    "        self.date = None\n",
    "        self.data = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.dates\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"CLIMATE Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.date = datetime.strptime(key.get(\"date\"), \"%Y-%m-%d\")      # str -> date\n",
    "            self.date = datetime.combine(self.date, datetime.min.time())    # date -> datetime\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.date and self.data:\n",
    "            print(\"CLIMATE Process Done\")\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error in closing Climate Writer: \" + str(err))\n",
    "\n",
    "        if self.date and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": self.date,\n",
    "                \"climate\": {\n",
    "                    \"air_temperature\": self.data.get(\"air_temperature_celcius\"),\n",
    "                    \"ghi\": self.data.get(\"GHI_w/m2\"),\n",
    "                    \"max_wind_speed\": self.data.get(\"max_wind_speed\"),\n",
    "                    \"precipitation\": self.data.get(\"precipitation\"),\n",
    "                    \"relative_humidity\": self.data.get(\"humidity\"),\n",
    "                    \"windspeed_knots\": self.data.get(\"windspeed_knots\")\n",
    "                },\n",
    "                \"hotspots\": []\n",
    "            }\n",
    "            #local_hotspots.clear()\n",
    "\n",
    "            try:\n",
    "                self.col.insert_one(db_obj)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting CLIMATE data to DB: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"CLIMATE data inserted\")\n",
    "                print(\"Hotspots count \" + str(len(db_obj.get(\"hotspots\"))))\n",
    "                print(\"Collection Size: \" + str(self.col.count_documents({})))\n",
    "                print(\"---------------------------\")\n",
    "            finally:\n",
    "                self.client.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T13:21:20.089729Z",
     "start_time": "2023-06-05T13:21:20.082938Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "class HotspotWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.client = None\n",
    "        self.data = None\n",
    "        self.datetime = None\n",
    "        self.producer = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.hotspots\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"HOTSPOT Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.datetime = datetime.strptime(key.get(\"datetime\"), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.datetime and self.data:\n",
    "            print(\"Process Done\")\n",
    "\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error: \" + str(err))\n",
    "\n",
    "        if self.datetime and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": ObjectId(),\n",
    "                \"confidence\": self.data.get(\"confidence\"),\n",
    "                \"datetime\": self.datetime,\n",
    "                \"date\": datetime.combine(self.datetime.date(), datetime.min.time()),\n",
    "                \"lat\": self.data.get(\"latitude\"),\n",
    "                \"lng\": self.data.get(\"longitude\"),\n",
    "                \"surface_temperature\": self.data.get(\"surface_temperature_celcius\")\n",
    "            }\n",
    "            try:\n",
    "                local_hotspots.append(db_obj)\n",
    "                print(id(local_hotspots))\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting HOTSPOT data to Memory: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"HOTSPOT data inserted to local memory\")\n",
    "                print(\"Local HOTSPOT data size: \" + str(len(local_hotspots)))\n",
    "                print(\"---------------------------\")\n",
    "            finally:\n",
    "                self.client.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T13:21:20.667651Z",
     "start_time": "2023-06-05T13:21:20.660267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "climate_writer = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(continuous=\"10 seconds\")\n",
    "    .foreach(ClimateWriter())\n",
    ")\n",
    "\n",
    "aqua_writer = (\n",
    "    p2_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(continuous=\"2 seconds\")\n",
    "    .foreach(HotspotWriter())\n",
    ")\n",
    "\n",
    "terra_writer = (\n",
    "    p3_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(continuous=\"2 seconds\")\n",
    "    .foreach(HotspotWriter())\n",
    ")\n",
    "\n",
    "console_logger = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('complete')\n",
    "    .format('console')\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T13:21:23.788467Z",
     "start_time": "2023-06-05T13:21:22.972838Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 23:21:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-aa7d9684-8aeb-47aa-9d58-21d11322f746. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 23:21:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 23:21:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-5550a8c4-9255-443a-9d16-d20d3b214f4b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 23:21:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 23:21:25 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:25 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:25 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 23:21:25 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 23:21:25 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-91d9e7ad-93b0-4ca3-a2a3-22a609690b11. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 23:21:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 23:21:26 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "[Stage 410:>  (0 + 1) / 1][Stage 411:>  (0 + 1) / 1][Stage 412:>  (0 + 1) / 1]\r]HOTSPOT Processing\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "Process Done\n",
      "HOTSPOT Processing\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "Process Done\n",
      "140629715726208\n",
      "---------------------------\n",
      "140629715726208HOTSPOT data inserted to local memory\n",
      "\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memoryLocal HOTSPOT data size: 1\n",
      "\n",
      "---------------------------Local HOTSPOT data size: 1\n",
      "\n",
      "---------------------------\n",
      "CLIMATE Processing                                                              \n",
      "CLIMATE Process Done\n",
      "---------------------------\n",
      "CLIMATE data inserted\n",
      "Hotspots count 0\n",
      "Collection Size: 216\n",
      "---------------------------\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 414:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "Process Done\n",
      "HOTSPOT Processing\n",
      "140630001512640\n",
      "---------------------------\n",
      "Process Done\n",
      "140629724360896\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "HOTSPOT ProcessingHOTSPOT Processing                                            \n",
      "\n",
      "Process Done\n",
      "140629724339456\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "Process Done\n",
      "140629443970112\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 418:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "Process Done\n",
      "140630001491200\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "140629715726208\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 420:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "140629443970112\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "Process Done\n",
      "140629443970112\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.errors import StreamingQueryException\n",
    "\n",
    "queries = []\n",
    "try:\n",
    "    queries.append(climate_writer.start())\n",
    "    queries.append(aqua_writer.start())\n",
    "    queries.append(terra_writer.start())\n",
    "    for query in queries:\n",
    "        query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    for query in queries:\n",
    "        query.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T13:21:42.437911Z",
     "start_time": "2023-06-05T13:21:24.339529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:04:16.464245Z",
     "start_time": "2023-06-05T09:04:16.462215Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
