{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:48.484584Z",
     "start_time": "2023-06-05T09:12:48.182103Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from json import loads, JSONDecodeError\n",
    "from datetime import datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\"\"\"\n",
    "    Convert lat-lng from tuple of floating points to tuple of binary string presentations of the values\n",
    "    !!!IMPORTANT!!!\n",
    "        To avoid converting floating points to binary directly, all values are x100 and converted to integer\n",
    "        THE FINAL VALUE IS 100X THE ORIGINAL VALUE (but it doesnt matter for our use case)\n",
    "    !!!END OF IMPORTANT!!!\n",
    "\"\"\"\n",
    "def latlng_to_binstr(lat_lng: Tuple[float, float]) -> Tuple[str, str]:\n",
    "    # float(25.125), float(10.13) -> int(25125), int(10130) -> '0b10101100`', '0b11011010' -> '10101100', '11011010'\n",
    "    return bin(int(lat_lng[0]*100))[2:], bin(int(lat_lng[1]*100))[2:]\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are close together\n",
    "                                                          within 3 precision\n",
    "\"\"\"\n",
    "def are_close(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(3):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are the same\n",
    "                                                          within 5 precision\n",
    "\"\"\"\n",
    "def are_same(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(5):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:49.903579Z",
     "start_time": "2023-06-05T09:12:49.892785Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "TOPIC_NAME = \"topic_1\"\n",
    "HOST_IP = \"192.168.20.6\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:12:50.812405Z",
     "start_time": "2023-06-05T09:12:50.810693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/petermok/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/petermok/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-867621aa-051a-43d2-ae91-5a245bf1ce7d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 1446ms :: artifacts dl 135ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-867621aa-051a-43d2-ae91-5a245bf1ce7d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/40ms)\n",
      "23/06/05 19:12:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/06/05 19:12:57 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    EXCEPT errno 61 connection refused:\n",
    "        RESTART ipynb kernel\n",
    "\"\"\"\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:00.368818Z",
     "start_time": "2023-06-05T09:12:51.428138Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_topic_name = \"climate_producer\"\n",
    "p1_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p1_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p1_stream_df.printSchema()\n",
    "\n",
    "p1_output_stream_df = (\n",
    "    p1_stream_df\n",
    "    .select(p1_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p1_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:15.783387Z",
     "start_time": "2023-06-05T09:13:12.379072Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2_topic_name = \"aqua_producer\"\n",
    "p2_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p2_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p2_stream_df.printSchema()\n",
    "\n",
    "p2_output_stream_df = (\n",
    "    p2_stream_df\n",
    "    .select(p2_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p2_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:17.103367Z",
     "start_time": "2023-06-05T09:13:16.989060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p3_topic_name = \"terra_producer\"\n",
    "p3_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p3_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p3_stream_df.printSchema()\n",
    "\n",
    "p3_output_stream_df = (\n",
    "    p3_stream_df\n",
    "    .select(p3_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p3_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:13:18.919084Z",
     "start_time": "2023-06-05T09:13:18.788503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "class ClimateWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.producer = None\n",
    "        self.date = None\n",
    "        self.data = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.dates\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"CLIMATE Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.date = datetime.strptime(key.get(\"date\"), \"%Y-%m-%d\")      # str -> date\n",
    "            self.date = datetime.combine(self.date, datetime.min.time())    # date -> datetime\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.date and self.data:\n",
    "            print(\"CLIMATE Process Done\")\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error in closing Climate Writer: \" + str(err))\n",
    "\n",
    "        if self.date and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": self.date,\n",
    "                \"climate\": {\n",
    "                    \"air_temperature\": self.data.get(\"air_temperature_celcius\"),\n",
    "                    \"ghi\": self.data.get(\"GHI_w/m2\"),\n",
    "                    \"max_wind_speed\": self.data.get(\"max_wind_speed\"),\n",
    "                    \"precipitation\": self.data.get(\"precipitation\"),\n",
    "                    \"relative_humidity\": self.data.get(\"humidity\"),\n",
    "                    \"windspeed_knots\": self.data.get(\"windspeed_knots\")\n",
    "                },\n",
    "                \"hotspots\": []\n",
    "            }\n",
    "            #local_hotspots.clear()\n",
    "\n",
    "            try:\n",
    "                self.col.insert_one(db_obj)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting CLIMATE data to DB: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"CLIMATE data inserted\")\n",
    "                print(\"Hotspots count \" + str(len(db_obj.get(\"hotspots\"))))\n",
    "                print(\"Collection Size: \" + str(self.col.count_documents({})))\n",
    "                print(\"---------------------------\")\n",
    "            finally:\n",
    "                self.client.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:30:22.636060Z",
     "start_time": "2023-06-05T11:30:22.606096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "class HotspotWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.client = None\n",
    "        self.data = None\n",
    "        self.datetime = None\n",
    "        self.producer = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.hotspots\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"HOTSPOT Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.datetime = datetime.strptime(key.get(\"datetime\"), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.datetime and self.data:\n",
    "            print(\"Process Done\")\n",
    "\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error: \" + str(err))\n",
    "\n",
    "        if self.datetime and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": ObjectId(),\n",
    "                \"confidence\": self.data.get(\"confidence\"),\n",
    "                \"datetime\": self.datetime,\n",
    "                \"date\": datetime.combine(self.datetime.date(), datetime.min.time()),\n",
    "                \"lat\": self.data.get(\"latitude\"),\n",
    "                \"lng\": self.data.get(\"longitude\"),\n",
    "                \"surface_temperature\": self.data.get(\"surface_temperature_celcius\")\n",
    "            }\n",
    "            try:\n",
    "                print(id(local_hotspots))\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting HOTSPOT data to Memory: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"HOTSPOT data inserted to local memory\")\n",
    "                print(\"Local HOTSPOT data size: \" + str(len(local_hotspots)))\n",
    "                print(\"---------------------------\")\n",
    "            finally:\n",
    "                self.client.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:30:23.174663Z",
     "start_time": "2023-06-05T11:30:23.168058Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/serializers.py:459\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcloudpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPickleError:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     70\u001B[0m cp \u001B[38;5;241m=\u001B[39m CloudPickler(\n\u001B[1;32m     71\u001B[0m     file, protocol\u001B[38;5;241m=\u001B[39mprotocol, buffer_callback\u001B[38;5;241m=\u001B[39mbuffer_callback\n\u001B[1;32m     72\u001B[0m )\n\u001B[0;32m---> 73\u001B[0m \u001B[43mcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:632\u001B[0m, in \u001B[0;36mCloudPickler.dump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 632\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [142]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m climate_writer \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      2\u001B[0m     p1_output_stream_df\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mforeach(ClimateWriter())\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m aqua_writer \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 9\u001B[0m     \u001B[43mp2_output_stream_df\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutputMode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforeach\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHotspotWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m )\n\u001B[1;32m     15\u001B[0m terra_writer \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     16\u001B[0m     p3_output_stream_df\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39mforeach(HotspotWriter())\n\u001B[1;32m     20\u001B[0m )\n\u001B[1;32m     22\u001B[0m console_logger \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     23\u001B[0m     p1_output_stream_df\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     27\u001B[0m )\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/sql/streaming/readwriter.py:1254\u001B[0m, in \u001B[0;36mDataStreamWriter.foreach\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1251\u001B[0m     func \u001B[38;5;241m=\u001B[39m func_with_open_process_close  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[1;32m   1253\u001B[0m serializer \u001B[38;5;241m=\u001B[39m AutoBatchedSerializer(CPickleSerializer())\n\u001B[0;32m-> 1254\u001B[0m wrapped_func \u001B[38;5;241m=\u001B[39m \u001B[43m_wrap_function\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1255\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1256\u001B[0m jForeachWriter \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1257\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mexecution\u001B[38;5;241m.\u001B[39mpython\u001B[38;5;241m.\u001B[39mPythonForeachWriter(\n\u001B[1;32m   1258\u001B[0m         wrapped_func, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mschema()\n\u001B[1;32m   1259\u001B[0m     )\n\u001B[1;32m   1260\u001B[0m )\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/rdd.py:5241\u001B[0m, in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, deserializer, serializer, profiler)\u001B[0m\n\u001B[1;32m   5239\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m serializer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserializer should not be empty\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   5240\u001B[0m command \u001B[38;5;241m=\u001B[39m (func, profiler, deserializer, serializer)\n\u001B[0;32m-> 5241\u001B[0m pickled_command, broadcast_vars, env, includes \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_for_python_RDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5242\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   5243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSimplePythonFunction(\n\u001B[1;32m   5244\u001B[0m     \u001B[38;5;28mbytearray\u001B[39m(pickled_command),\n\u001B[1;32m   5245\u001B[0m     env,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5250\u001B[0m     sc\u001B[38;5;241m.\u001B[39m_javaAccumulator,\n\u001B[1;32m   5251\u001B[0m )\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/rdd.py:5224\u001B[0m, in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   5221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prepare_for_python_RDD\u001B[39m(sc: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkContext\u001B[39m\u001B[38;5;124m\"\u001B[39m, command: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mbytes\u001B[39m, Any, Any, Any]:\n\u001B[1;32m   5222\u001B[0m     \u001B[38;5;66;03m# the serialized command will be compressed by broadcast\u001B[39;00m\n\u001B[1;32m   5223\u001B[0m     ser \u001B[38;5;241m=\u001B[39m CloudPickleSerializer()\n\u001B[0;32m-> 5224\u001B[0m     pickled_command \u001B[38;5;241m=\u001B[39m \u001B[43mser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5225\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   5226\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pickled_command) \u001B[38;5;241m>\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mgetBroadcastThreshold(sc\u001B[38;5;241m.\u001B[39m_jsc):  \u001B[38;5;66;03m# Default 1M\u001B[39;00m\n\u001B[1;32m   5227\u001B[0m         \u001B[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/serializers.py:469\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    467\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not serialize object: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (e\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, emsg)\n\u001B[1;32m    468\u001B[0m print_exec(sys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[0;32m--> 469\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPicklingError(msg)\n",
      "\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: TypeError: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "climate_writer = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(ClimateWriter())\n",
    ")\n",
    "\n",
    "aqua_writer = (\n",
    "    p2_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(HotspotWriter())\n",
    ")\n",
    "\n",
    "terra_writer = (\n",
    "    p3_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(HotspotWriter())\n",
    ")\n",
    "\n",
    "console_logger = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('complete')\n",
    "    .format('console')\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:30:25.112474Z",
     "start_time": "2023-06-05T11:30:24.333795Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 21:27:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-e723fdc8-1d47-4812-a466-ab9682cabe1c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 21:27:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 21:27:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-0ab7b3fd-b828-473a-9b79-c55f0b8ba1ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 21:27:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-1d7a3c7c-c8f3-403c-a042-3e5021b82f9b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 21:27:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 21:27:27 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 277:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "140629726067456\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "Process Done\n",
      "140629443971584\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "CLIMATE Processing                                                  (0 + 1) / 1]\n",
      "CLIMATE Process Done\n",
      "---------------------------\n",
      "CLIMATE data inserted\n",
      "Hotspots count 0\n",
      "Collection Size: 201\n",
      "---------------------------\n",
      "HOTSPOT Processing                                                              \n",
      "Process Done\n",
      "HOTSPOT Processing\n",
      "140629719908608\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "Process Done\n",
      "140629724931584\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 282:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "140629444061696\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "Process Done\n",
      "140629175726208\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 284:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "140629444062464\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "Process Done\n",
      "140629452352576\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "------------------------------------------------------\n",
      "\n",
      "HOTSPOT Processing                                                  (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "140629991628928\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------Process Done\n",
      "\n",
      "140629991566848\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 0\n",
      "---------------------------\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 21:27:39 WARN Shell: Interrupted while joining on: Thread[Thread-7079,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1303)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1371)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1574)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$markMicroBatchEnd$1(MicroBatchExecution.scala:780)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchEnd(MicroBatchExecution.scala:780)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:749)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:802)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:747)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.errors import StreamingQueryException\n",
    "\n",
    "queries = []\n",
    "try:\n",
    "    queries.append(climate_writer.start())\n",
    "    queries.append(aqua_writer.start())\n",
    "    queries.append(terra_writer.start())\n",
    "    for query in queries:\n",
    "        query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    for query in queries:\n",
    "        query.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:27:39.914147Z",
     "start_time": "2023-06-05T11:27:26.970605Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T09:04:16.464245Z",
     "start_time": "2023-06-05T09:04:16.462215Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
