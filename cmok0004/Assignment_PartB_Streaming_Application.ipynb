{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T06:33:32.522062Z",
     "start_time": "2023-06-04T06:33:32.512411Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at, when\n",
    "from pprint import pprint\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "TOPIC_NAME = \"topic_1\"\n",
    "HOST_IP = \"192.168.20.6\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T06:33:32.522518Z",
     "start_time": "2023-06-04T06:33:32.516359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T06:33:32.724496Z",
     "start_time": "2023-06-04T06:33:32.524326Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_topic_name = \"producer_1\"\n",
    "p1_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p1_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p1_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T06:33:32.920771Z",
     "start_time": "2023-06-04T06:33:32.727801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_output_stream_df = (\n",
    "    p1_stream_df\n",
    "    .select(p1_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p1_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T06:33:32.992995Z",
     "start_time": "2023-06-04T06:33:32.924560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "\n",
    "class ClimateWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.producer = None\n",
    "        self.date = None\n",
    "        self.data = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        print(\"Opening Mongo Client\")\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.dates\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.date = key.get(\"date\")\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.date and self.data:       # insert stream data to db\n",
    "            self.write_to_db()\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error: \" + str(err))\n",
    "        print(\"Closing Mongo Client\")\n",
    "        self.client.close()\n",
    "\n",
    "    def write_to_db(self):\n",
    "        climate_db_obj = {\n",
    "            \"_id\": self.date,\n",
    "            \"climate\": {    # date-climate is 1-1\n",
    "                \"air_temperature\": int(self.data.get(\"air_temperature_celcius\")),\n",
    "                \"relative_humidity\": float(self.data.get(\"relative_humidity\")),\n",
    "                \"windspeed_knots\": float(self.data.get(\"windspeed_knots\")),\n",
    "                \"max_wind_speed\": float(self.data.get(\"max_wind_speed\")),\n",
    "                \"precipitation\": {\n",
    "                        \"flag\": self.data.get(\"precipitation\")[-1],\n",
    "                        \"value\": float(self.data.get(\"precipitation\")[:-1])\n",
    "                },\n",
    "                \"ghi\": int(self.data.get(\"GHI_w/m2\"))\n",
    "            },\n",
    "            \"hotspots\": []  # init empty hotspots\n",
    "        }\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T07:10:19.639715Z",
     "start_time": "2023-06-04T07:10:19.636064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "class HotspotWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = None\n",
    "        self.mongo_client = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.mongo_client.fit3182_assignment_db\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "\n",
    "        self.db[TOPIC_NAME].insert_one(row.asDict())\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T07:10:20.072603Z",
     "start_time": "2023-06-04T07:10:20.051079Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py:503\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 503\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msendall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mBrokenPipeError\u001B[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPy4JNetworkError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py:506\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    505\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError while sending or receiving.\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 506\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JNetworkError(\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError while sending\u001B[39m\u001B[38;5;124m\"\u001B[39m, e, proto\u001B[38;5;241m.\u001B[39mERROR_ON_SEND)\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mPy4JNetworkError\u001B[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Input \u001B[0;32mIn [199]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m climate_writer \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mp1_output_stream_df\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mforeach(ClimateWriter())\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m console_logger \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m      9\u001B[0m     p1_output_stream_df\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     13\u001B[0m )\n\u001B[1;32m     15\u001B[0m writer \u001B[38;5;241m=\u001B[39m climate_writer\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/sql/dataframe.py:541\u001B[0m, in \u001B[0;36mDataFrame.writeStream\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwriteStream\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataStreamWriter:\n\u001B[1;32m    514\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;124;03m    Interface for saving the content of the streaming :class:`DataFrame` out into external\u001B[39;00m\n\u001B[1;32m    516\u001B[0m \u001B[38;5;124;03m    storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m    <pyspark.sql.streaming.query.StreamingQuery object at 0x...>\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataStreamWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/pyspark/sql/streaming/readwriter.py:739\u001B[0m, in \u001B[0;36mDataStreamWriter.__init__\u001B[0;34m(self, df)\u001B[0m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df \u001B[38;5;241m=\u001B[39m df\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39msparkSession\n\u001B[0;32m--> 739\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwriteStream\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py:1053\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(retry, connection, pne):\n\u001B[1;32m   1052\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException while sending command.\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 1053\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1055\u001B[0m     logging\u001B[38;5;241m.\u001B[39mexception(\n\u001B[1;32m   1056\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException while sending command.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "climate_writer = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(ClimateWriter())\n",
    ")\n",
    "\n",
    "console_logger = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n",
    "\n",
    "writer = climate_writer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T11:07:29.879729Z",
     "start_time": "2023-06-04T11:07:28.472634Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "('10011101000', '110010001011')"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\"\"\"\n",
    "    Convert lat-lng from tuple of floating points to tuple of binary string presentations of the values\n",
    "    !!!IMPORTANT!!!\n",
    "    To avoid converting floating points to binary directly, all values are x100 and converted to integer\n",
    "    THE FINAL VALUE IS 100X THE ORIGINAL VALUE (but it doesnt matter for our use case)\n",
    "\"\"\"\n",
    "def latlng_to_binstr(lat_lng: Tuple[float, float]) -> Tuple[str, str]:\n",
    "    # float(25.125), float(10.13) -> int(25125), int(10130) -> '0b10101100`', '0b11011010' -> '10101100', '11011010'\n",
    "    return bin(int(lat_lng[0]*100))[2:], bin(int(lat_lng[1]*100))[2:]\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are close together\n",
    "                                                          within 3 precision\n",
    "\"\"\"\n",
    "def are_close(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are the same\n",
    "                                                          within 5 precision\n",
    "\"\"\"\n",
    "def are_same(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    pass\n",
    "\n",
    "latlng_to_binstr((12.561, 32.12))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T11:33:57.909510Z",
     "start_time": "2023-06-04T11:33:57.880579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 17:10:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-68706a2d-50ff-47ee-9dbf-f4d29d3563b0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/04 17:10:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/04 17:10:21 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 17:10:21 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/04 17:10:21 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/04 17:10:21 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/04 17:10:21 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "Opening Mongo Client                                                (0 + 1) / 1]\n",
      "Closing Mongo Client\n",
      "Opening Mongo Client                                                (0 + 1) / 1]\n",
      "Processing\n",
      "140451876642144\n",
      "2024-08-01\n",
      "{'latitude': -36.404, 'longitude': 142.5467, 'air_temperature_celcius': 25, 'relative_humidity': 60.9, 'windspeed_knots': 6.3, 'max_wind_speed': 8.9, 'precipitation ': ' 0.00I', 'GHI_w/m2': 195}\n",
      "Process Over\n",
      "Processing\n",
      "Process skipped: \n",
      "Extra data: line 1 column 20 (char 19) in decoding key (Don't worry about it, it works 50% of the time)\n",
      "Process skipped: \n",
      "Expecting value: line 1 column 1 (char 0) in decoding key (Don't worry about it, it works 50% of the time)\n",
      "140451876642144\n",
      "2024-08-01\n",
      "{'latitude': -36.404, 'longitude': 142.5467, 'air_temperature_celcius': 25, 'relative_humidity': 60.9, 'windspeed_knots': 6.3, 'max_wind_speed': 8.9, 'precipitation ': ' 0.00I', 'GHI_w/m2': 195}\n",
      "Process Over\n",
      "Closing Mongo Client\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "23/06/04 17:10:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@b3a7744] is aborting.\n",
      "23/06/04 17:10:38 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@b3a7744] aborted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from pyspark.errors import StreamingQueryException\n",
    "\n",
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "    sleep(10)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    query.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T07:10:38.451040Z",
     "start_time": "2023-06-04T07:10:20.921396Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
