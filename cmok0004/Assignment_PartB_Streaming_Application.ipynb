{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:43:25.733694Z",
     "start_time": "2023-06-04T17:43:25.730371Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from json import loads, JSONDecodeError\n",
    "from datetime import datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\"\"\"\n",
    "    Convert lat-lng from tuple of floating points to tuple of binary string presentations of the values\n",
    "    !!!IMPORTANT!!!\n",
    "        To avoid converting floating points to binary directly, all values are x100 and converted to integer\n",
    "        THE FINAL VALUE IS 100X THE ORIGINAL VALUE (but it doesnt matter for our use case)\n",
    "    !!!END OF IMPORTANT!!!\n",
    "\"\"\"\n",
    "def latlng_to_binstr(lat_lng: Tuple[float, float]) -> Tuple[str, str]:\n",
    "    # float(25.125), float(10.13) -> int(25125), int(10130) -> '0b10101100`', '0b11011010' -> '10101100', '11011010'\n",
    "    return bin(int(lat_lng[0]*100))[2:], bin(int(lat_lng[1]*100))[2:]\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are close together\n",
    "                                                          within 3 precision\n",
    "\"\"\"\n",
    "def are_close(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(3):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\"\"\"\n",
    "    Return True if two given latitude-longitude pairs are the same\n",
    "                                                          within 5 precision\n",
    "\"\"\"\n",
    "def are_same(lat_lng_1: Tuple[float, float], lat_lng_2: Tuple[float, float]) -> bool:\n",
    "    lat_lng_1 = latlng_to_binstr(lat_lng_1)\n",
    "    lat_lng_2 = latlng_to_binstr(lat_lng_2)\n",
    "    for i in range(5):\n",
    "        if lat_lng_1[0][i] != lat_lng_2[0][i] or lat_lng_1[1][i] != lat_lng_2[1][i]:\n",
    "            return False\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.115418Z",
     "start_time": "2023-06-04T17:19:17.091989Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "TOPIC_NAME = \"topic_1\"\n",
    "HOST_IP = \"192.168.20.6\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.116216Z",
     "start_time": "2023-06-04T17:19:17.106851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    EXCEPT errno 61 connection refused:\n",
    "        RESTART ipynb kernel\n",
    "\"\"\"\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('[Demo] Spark Streaming from Kafka into MongoDB')\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.272916Z",
     "start_time": "2023-06-04T17:19:17.107089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_topic_name = \"climate_producer\"\n",
    "p1_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p1_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p1_stream_df.printSchema()\n",
    "\n",
    "p1_output_stream_df = (\n",
    "    p1_stream_df\n",
    "    .select(p1_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p1_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.460590Z",
     "start_time": "2023-06-04T17:19:17.276740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2_topic_name = \"aqua_producer\"\n",
    "p2_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p2_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p2_stream_df.printSchema()\n",
    "\n",
    "p2_output_stream_df = (\n",
    "    p2_stream_df\n",
    "    .select(p2_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p2_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.586253Z",
     "start_time": "2023-06-04T17:19:17.471982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p3_topic_name = \"terra_producer\"\n",
    "p3_stream_df = (\n",
    "    spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{HOST_IP}:9092')\n",
    "    .option('subscribe', p3_topic_name)\n",
    "    .load()\n",
    ")\n",
    "p3_stream_df.printSchema()\n",
    "\n",
    "p3_output_stream_df = (\n",
    "    p3_stream_df\n",
    "    .select(p3_stream_df.columns[:2])   # get column of key (producer_id, date) and value (data)\n",
    ")\n",
    "p3_output_stream_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:17.686654Z",
     "start_time": "2023-06-04T17:19:17.587996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "class ClimateWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.producer = None\n",
    "        self.date = None\n",
    "        self.data = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        print(\"Opening Mongo Client\")\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.dates\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"CLIMATE Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.date = datetime.strptime(key.get(\"date\"), \"%Y-%m-%d\")      # str -> date\n",
    "            self.date = datetime.combine(self.date, datetime.min.time())    # date -> datetime\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"CLIMATE Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.date and self.data:\n",
    "            print(\"CLIMATE Process Done\")\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error in closing Climate Writer: \" + str(err))\n",
    "\n",
    "        if self.date and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": self.date,\n",
    "                \"climate\": {\n",
    "                    \"air_temperature\": self.data.get(\"air_temperature_celcius\"),\n",
    "                    \"ghi\": self.data.get(\"GHI_w/m2\"),\n",
    "                    \"max_wind_speed\": self.data.get(\"max_wind_speed\"),\n",
    "                    \"precipitation\": self.data.get(\"precipitation\"),\n",
    "                    \"relative_humidity\": self.data.get(\"humidity\"),\n",
    "                    \"windspeed_knots\": self.data.get(\"windspeed_knots\")\n",
    "                },\n",
    "                \"hotspots\": [hotspot.get(\"_id\") for hotspot in self.local_hotspots]  # length of this list ranges from 0 to 5\n",
    "            }\n",
    "            try:\n",
    "                self.col.insert_one(db_obj)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting CLIMATE data to DB: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"CLIMATE data inserted\")\n",
    "                print(\"Hotspots count \" + str(len(db_obj.get(\"hotspots\"))))\n",
    "                print(\"Collection Size: \" + str(self.col.count_documents({})))\n",
    "                print(\"---------------------------\")\n",
    "                self.local_hotspots = []\n",
    "            finally:\n",
    "                self.client.close()\n",
    "                print(\"CLIMATE Closed Mongo Client\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:49:23.234533Z",
     "start_time": "2023-06-04T17:49:23.211572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class HotspotWriter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.db = None\n",
    "        self.col = None\n",
    "        self.client = None\n",
    "        self.data = None\n",
    "        self.datetime = None\n",
    "        self.producer = None\n",
    "\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.client = MongoClient(\n",
    "            host=f'{\"localhost\"}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.client.fit3182_assignment_db\n",
    "        self.col = self.db.hotspots\n",
    "        return True\n",
    "\n",
    "    # called once per row of the result dataframe\n",
    "    def process(self, row):\n",
    "        print(\"HOTSPOT Processing\")\n",
    "        key = row[\"key\"].decode()\n",
    "        value = row[\"value\"].decode()\n",
    "        try:\n",
    "            key = dict(loads(key.replace(\"\\'\", \"\\\"\")))      # dict-in-str -> json -> dict\n",
    "            self.producer = key.get(\"producer\")\n",
    "            self.datetime = datetime.strptime(key.get(\"datetime\"), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "        try:\n",
    "            value = dict(loads(value.replace(\"\\'\", \"\\\"\")))  # dict-in-str -> json -> dict\n",
    "            self.data = value\n",
    "        except JSONDecodeError as e:\n",
    "            print(\"Process skipped: \\n\" + str(e) + \" in decoding key (Don't worry about it, it works 50% of the time)\")\n",
    "\n",
    "        if self.producer and self.datetime and self.data:\n",
    "            print(\"Process Done\")\n",
    "\n",
    "\n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        if err:\n",
    "            print(\"Error: \" + str(err))\n",
    "\n",
    "        if self.datetime and self.data:\n",
    "            db_obj = {\n",
    "                \"_id\": ObjectId(),\n",
    "                \"confidence\": self.data.get(\"confidence\"),\n",
    "                \"datetime\": self.datetime,\n",
    "                \"date\": datetime.combine(self.datetime.date(), datetime.min.time()),\n",
    "                \"lat\": self.data.get(\"latitude\"),\n",
    "                \"lng\": self.data.get(\"longitude\"),\n",
    "                \"surface_temperature\": self.data.get(\"surface_temperature\")\n",
    "            }\n",
    "            try:\n",
    "                self.local_hotspots.append(db_obj)\n",
    "            except Exception as e:\n",
    "                print(\"Exception in inserting HOTSPOT data to DB: \" + str(e))\n",
    "            else:\n",
    "                print(\"---------------------------\")\n",
    "                print(\"HOTSPOT data inserted to local memory\")\n",
    "                print(\"Local HOTSPOT data size: \" + str(len(self.local_hotspots)))\n",
    "                print(\"---------------------------\")\n",
    "            finally:\n",
    "                self.client.close()\n",
    "                print(\"CLIMATE Closed Mongo Client\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:49:24.875060Z",
     "start_time": "2023-06-04T17:49:24.867967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "local_hotspots = []\n",
    "\n",
    "climate_writer = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(ClimateWriter(local_hotspots))\n",
    ")\n",
    "\n",
    "aqua_writer = (\n",
    "    p2_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(HotspotWriter(local_hotspots))\n",
    ")\n",
    "\n",
    "terra_writer = (\n",
    "    p3_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreach(HotspotWriter(local_hotspots))\n",
    ")\n",
    "\n",
    "console_logger = (\n",
    "    p1_output_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:49:25.518245Z",
     "start_time": "2023-06-04T17:49:25.290038Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 03:49:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-f4ca9be6-6f22-4e52-a7df-1dafc3449d84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 03:49:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 03:49:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-243cb0f1-6005-45e7-ae34-744ff767bf84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 03:49:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nk/pq_9ypcs6_x5jdx99mrtszc80000gn/T/temporary-b1cce8df-012c-47cb-8ddc-444a28bb953b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/05 03:49:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 03:49:26 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "23/06/05 03:49:27 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:27 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "23/06/05 03:49:27 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "23/06/05 03:49:27 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "23/06/05 03:49:27 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "Opening Mongo Client) / 1][Stage 168:>  (0 + 1) / 1][Stage 169:>  (0 + 1) / 1]1]\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 171:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing                                                  (0 + 0) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 175:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing                                                              \n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT ProcessingHOTSPOT Processing:>  (0 + 1) / 1][Stage 180:>  (0 + 1) / 1]1]\n",
      "\n",
      "Process DoneProcess Done\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "HOTSPOT data inserted to local memoryHOTSPOT data inserted to local memory\n",
      "\n",
      "Local HOTSPOT data size: 1Local HOTSPOT data size: 1\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "CLIMATE Closed Mongo Client\n",
      "CLIMATE Closed Mongo Client\n",
      "Opening Mongo Client                                                            \n",
      "CLIMATE Processing\n",
      "CLIMATE Process Done\n",
      "---------------------------\n",
      "CLIMATE data inserted\n",
      "Hotspots count 0\n",
      "Collection Size: 178\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 182:>                (0 + 1) / 1]\n",
      "HOTSPOT ProcessingProcess Done\n",
      "\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo ClientProcess Done\n",
      "\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing                                                              \n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 186:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 188:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "HOTSPOT ProcessingCLIMATE Closed Mongo Client\n",
      "\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Opening Mongo Client                                                (0 + 1) / 1]\n",
      "CLIMATE Processing\n",
      "CLIMATE Process Done\n",
      "---------------------------Stage 190:>  (0 + 1) / 1][Stage 191:>  (0 + 1) / 1]\n",
      "CLIMATE data inserted\n",
      "Hotspots count 0\n",
      "Collection Size: 179\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 193:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing                                                              \n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 195:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 197:>                (0 + 1) / 1]\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing          (0 + 1) / 1][Stage 199:>                (0 + 1) / 1]\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "Opening Mongo Client) / 1][Stage 201:>  (0 + 1) / 1][Stage 202:>  (0 + 1) / 1]1]\n",
      "CLIMATE Processing\n",
      "CLIMATE Process Done\n",
      "---------------------------\n",
      "CLIMATE data inserted\n",
      "Hotspots count 0\n",
      "Collection Size: 180\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing                                                              \n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "HOTSPOT Processing\n",
      "Process Done\n",
      "---------------------------\n",
      "HOTSPOT data inserted to local memory\n",
      "Local HOTSPOT data size: 1\n",
      "---------------------------\n",
      "CLIMATE Closed Mongo Client\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/petermok/opt/miniconda3/envs/fit3182/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopped query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 03:50:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@3f83c1cf] is aborting.\n",
      "23/06/05 03:50:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@3f83c1cf] aborted.\n",
      "23/06/05 03:50:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@5fda5623] is aborting.\n",
      "23/06/05 03:50:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.ForeachWrite$$anon$2@5fda5623] aborted.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.errors import StreamingQueryException\n",
    "\n",
    "queries = []\n",
    "try:\n",
    "    queries.append(climate_writer.start())\n",
    "    queries.append(aqua_writer.start())\n",
    "    queries.append(terra_writer.start())\n",
    "    for query in queries:\n",
    "        query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopped query')\n",
    "except StreamingQueryException as exc:\n",
    "    print(exc)\n",
    "finally:\n",
    "    for query in queries:\n",
    "        query.stop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:50:01.726198Z",
     "start_time": "2023-06-04T17:49:26.226041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-04T17:19:22.645637Z",
     "start_time": "2023-06-04T17:19:22.594670Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
